# Environment parameters

env: LearnToMove
random_seed: 2019
num_agents: 3
state_dims: 99
action_dims: 22
visualize: 0

# Training parameters

model: d3pg
batch_size: 256
num_steps_train: 1000 # number of episodes from all agents
max_ep_length: 1000 # maximum number of steps per episode
replay_mem_size: 1000000 # maximum capacity of replay memory
priority_alpha: 0.6 # controls the randomness vs prioritisation of the prioritised sampling (0.0 = Uniform sampling, 1.0 = Greedy prioritisation)
priority_beta_start: 0.4 # starting value of beta - controls to what degree IS weights influence the gradient updates to correct for the bias introduces by priority sampling (0 - no correction, 1 - full correction)
priority_beta_end: 1.0 # beta will be linearly annelaed from its start value to this value thoughout training
discount_rate: 0.99 # Discount rate (gamma) for future rewards
epsilon_mode: cyclic
epsilon_initial_value: 0.8
epsilon_final_value: 0.001
epsilon_num_cycles: 10
n_step_returns: 5 # number of future steps to collect experiences for N-step returns
update_agent_ep: 1 # agent gets latest parameters from learner every update_agent_ep episodes
replay_queue_size: 64 # queue with replays from all the agents
batch_queue_size: 64 # queue with batches given to learner
replay_memory_prioritized: 0 # flag to use prioritized buffer
device: cuda
num_episode_save: 500
analyze_replay_buffer: 0 # Perform analysis of reward and observation stats in the end
data_normalization_path:

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.0005
dense_size: 300 # size of the 2 hidden layers in networks
final_layer_init: 0.003
tau: 0.001 # parameter for soft target network updates
policy_output_nonlinearity: sigmoid

# Miscellaneous

results_path: results