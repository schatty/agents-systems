# Environment parameters

env: InvertedPendulum-v2
state_dim: 4
action_dim: 1
action_low: -1
action_high: 1
seed: 2019

# Training parameters

model: ddpg
batch_size: 256
num_steps_train: 1_000_000 # number of episodes from all agents
replay_mem_size: 1_000_000 # maximum capacity of replay memory
discount_rate: 0.99 # Discount rate (gamma) for future rewards
device: cuda
load_model:
start_timesteps: 10_000 # Time steps initial random policy is used
eval_freq: 5_000 # How often (time steps) we evaluate
expl_noise: 0.1 # Std of Gaussian exploration noise
policy_noise: 0.2 # Noise added to target policy during critic update
noise_clip: 0.5 # Range to clip target policy noise
policy_freq: 2 # Frequency of delayed policy updates

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.0005
dense_size: 400 # size of the 2 hidden layers in networks
final_layer_init: 0.003
tau: 0.001 # parameter for soft target network updates

# Miscellaneous

results_path: results


