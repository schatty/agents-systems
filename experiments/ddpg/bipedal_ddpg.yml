# Environment parameters

env: BipedalWalker-v2
state_dim: 24
action_dim: 4
action_low: -1
action_high: 1
random_seed: 2019

# Training parameters

model: ddpg
batch_size: 256
num_steps_train: 500_000 # number of episodes from all agents
start_timesteps: 100 # Timesteps without training
replay_mem_size: 1_000_000 # maximum capacity of replay memory
discount_rate: 0.99 # Discount rate (gamma) for future rewards
n_step_returns: 1 # number of future steps to collect experiences for N-step returns
replay_queue_size: 64 # queue with replays from all the agents
batch_queue_size: 64 # queue with batches given to learner
expl_noise: 0.2 # Std of Gaussian exploration noise
num_episode_save: 100
eval_freq: 1000 # How often we evaluate
device: cuda
agent_device: gpu
load_model: # Whether to load trained model
save_reward_threshold: 1 # difference in best reward to save agent model

# Network parameters

critic_learning_rate: 0.0005
actor_learning_rate: 0.0005
dense_size: 300 # size of the 2 hidden layers in networks
final_layer_init: 0.003
tau: 0.005 # parameter for soft target network updates

# Miscellaneous

results_path: results

